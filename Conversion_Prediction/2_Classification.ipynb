{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion Prediction : Classification\n",
    ">In this second phase of the project, we will try to:  \n",
    "\n",
    ">> find out which ML technique suits the best the data and the prediction goal and   \n",
    "make predictions given that technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4wtUjVlLIfw"
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Data Preparation](#section1)\n",
    "    * [1.1. Load Data](#section21)\n",
    "    * [1.2. Predictors and Target](#section21)\n",
    "    * [1.3. Training and Validation sets](#section22)\n",
    "    * [1.4. Preprocessing pipeline](#section23)\n",
    "* [2. Classification](#section2)\n",
    "    * [2.1. Preliminary Analysis](#section21)\n",
    "        * [2.1.1. Statmodels logit](#section21)\n",
    "        * [2.1.2. Preliminary model selection](#section22)\n",
    "    * [2.2. Logistic Regression](#section23)\n",
    "        * [2.2.1. Model Evaluation](#section24)\n",
    "    * [2.3. XGBoost Classifier](#section25)\n",
    "        * [2.3.1. Model Seletion](#section26)\n",
    "        * [2.3.2. Model Evaluation](#section27)\n",
    "    * [2.4. Train the chosen models on the whole dataset](#section25)\n",
    "* [3. Predict the target of the test set](#section2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv1phpoALIfx",
    "tags": []
   },
   "source": [
    " #### Import useful modules ⬇️⬇️ and Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eK8s9YTVLIfx"
   },
   "outputs": [],
   "source": [
    "# generic libs\n",
    "import os\n",
    "import pandas as pd\n",
    "from numpy import append\n",
    "from time import time\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML tools\n",
    "# pre_training tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# training tools\n",
    "import statsmodels.api as sm \n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# predefined modules\n",
    "from modules import MyFunctions as MyFunct\n",
    "\n",
    "# Global parameters \n",
    "train_filepath = 'data/conversion_data_train.csv'\n",
    "test_filepath = 'data/conversion_data_test.csv'\n",
    "results_path = \"results/\"\n",
    "\n",
    "if not os.path.exists(\"output\"):\n",
    "    os.mkdir(\"output\")\n",
    "output_path = 'output/'\n",
    "\n",
    "seed = 0\n",
    "cv = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HE-RhpScLIfz",
    "outputId": "4117338e-c644-42c2-f8a3-39129427e3bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "...Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = pd.read_csv(train_filepath)\n",
    "print(\"...Done.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_1qx9qVNLIf1"
   },
   "outputs": [],
   "source": [
    "# Separate target variable y from features X\n",
    "y = dataset['converted']\n",
    "X = dataset.drop('converted', axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8uS304LIf1"
   },
   "source": [
    "🗒 **_Stratify_**: If we select observations from the dataset with a uniform probability distribution (**stratify = y(dataset['converted']**), we will draw observations from each class with the same probability of their occurrence in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify = y)\n",
    "\n",
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">🗒 In the dataset, we have mixed data with both quantitative and qualitative predictors. Hence, we must define a different preprocessing pipeline for each category.\n",
    ">> 1. we will **standardize** the numerical data before training to eliminate large scales effect on the learning phase.\n",
    ">> 2. we will **encode** categorical predictors using one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "830CiJovLIf1"
   },
   "outputs": [],
   "source": [
    "# Create pipeline for numeric features \n",
    "#Num_X =['age', 'total_pages_visited'] \n",
    "num_X = [1,4]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create pipeline for categorical features\n",
    "#cat_X = ['country', 'new_user', 'source']\n",
    "cat_X = [0,2,3]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# Use ColumnTranformer to make a preprocessor object \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_X),\n",
    "        ('cat', categorical_transformer, cat_X)\n",
    "    ])\n",
    "\n",
    "# Preprocessings on train set (8 cols = 2 for numerci columns + 1 for new_user + 3 for country + 2 for source)\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test  = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46pmETdjLIf2"
   },
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46pmETdjLIf2"
   },
   "source": [
    "### Statmodels logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV9tpMO8LIf2"
   },
   "source": [
    "🗒 **_Statmodels_**: we want to establish a preliminary analysis using the Statmodels logit function that gives a detailed results of a regression model in order to confirm what we have noticed in the EDA part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "296BLakQLIf2"
   },
   "outputs": [],
   "source": [
    "cols =preprocessor.transformers_[1][1].named_steps['encoder'].get_feature_names().tolist()\n",
    "columns = ['const','age', 'total_pages_visited'] + cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "gU4xKy_mLIf2",
    "outputId": "b6643c9f-1595-457b-b105-9fd200bcd29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.040482\n",
      "         Iterations 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>227664</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>227655</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Thu, 14 Apr 2022</td> <th>  Pseudo R-squ.:     </th>  <td>0.7159</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>06:56:11</td>     <th>  Log-Likelihood:    </th> <td> -9216.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -32443.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>               <td>   -8.9262</td> <td>    0.149</td> <td>  -59.801</td> <td> 0.000</td> <td>   -9.219</td> <td>   -8.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                 <td>   -0.5954</td> <td>    0.023</td> <td>  -25.768</td> <td> 0.000</td> <td>   -0.641</td> <td>   -0.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_pages_visited</th> <td>    2.5540</td> <td>    0.025</td> <td>  103.431</td> <td> 0.000</td> <td>    2.506</td> <td>    2.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0_Germany</th>          <td>    3.7701</td> <td>    0.155</td> <td>   24.288</td> <td> 0.000</td> <td>    3.466</td> <td>    4.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0_UK</th>               <td>    3.6049</td> <td>    0.141</td> <td>   25.575</td> <td> 0.000</td> <td>    3.329</td> <td>    3.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x0_US</th>               <td>    3.2286</td> <td>    0.137</td> <td>   23.625</td> <td> 0.000</td> <td>    2.961</td> <td>    3.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1_1</th>                <td>   -1.6935</td> <td>    0.042</td> <td>  -40.495</td> <td> 0.000</td> <td>   -1.775</td> <td>   -1.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2_Direct</th>           <td>   -0.1985</td> <td>    0.057</td> <td>   -3.462</td> <td> 0.001</td> <td>   -0.311</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2_Seo</th>              <td>   -0.0450</td> <td>    0.047</td> <td>   -0.960</td> <td> 0.337</td> <td>   -0.137</td> <td>    0.047</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.32 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:               227664\n",
       "Model:                          Logit   Df Residuals:                   227655\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Thu, 14 Apr 2022   Pseudo R-squ.:                  0.7159\n",
       "Time:                        06:56:11   Log-Likelihood:                -9216.3\n",
       "converged:                       True   LL-Null:                       -32443.\n",
       "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
       "=======================================================================================\n",
       "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------\n",
       "const                  -8.9262      0.149    -59.801      0.000      -9.219      -8.634\n",
       "age                    -0.5954      0.023    -25.768      0.000      -0.641      -0.550\n",
       "total_pages_visited     2.5540      0.025    103.431      0.000       2.506       2.602\n",
       "x0_Germany              3.7701      0.155     24.288      0.000       3.466       4.074\n",
       "x0_UK                   3.6049      0.141     25.575      0.000       3.329       3.881\n",
       "x0_US                   3.2286      0.137     23.625      0.000       2.961       3.496\n",
       "x1_1                   -1.6935      0.042    -40.495      0.000      -1.775      -1.612\n",
       "x2_Direct              -0.1985      0.057     -3.462      0.001      -0.311      -0.086\n",
       "x2_Seo                 -0.0450      0.047     -0.960      0.337      -0.137       0.047\n",
       "=======================================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.32 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = sm.add_constant(X_train)\n",
    "\n",
    "logit = sm.Logit(y_train,X2)\n",
    "\n",
    "logit_fit = logit.fit()\n",
    "\n",
    "logit_fit.summary(xname=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmr5pI2ZLIf3"
   },
   "source": [
    "****************************************************************  \n",
    "> 🗒 **_Notation_**:  \n",
    "                  **x0_**: Country     **x1_1**: new_user    **x2_**: Source      \n",
    "                  \n",
    "> 🗒 **Statistical Significance (P>|z|)**: all variables are significant, except x2_Seo. it's normal as among the values of the initial variable source, there is no big difference on how each source may influence the user conversion and all say approximately the same thing\n",
    "\n",
    "> 🗒 **Predictors Importance (coef)**: looking at the coefficients of the regression, we can notice that the predictors are ordred as follows given their importance:     \n",
    "**Country, total_pages_visited, age, Source**\n",
    "\n",
    "> 🗒 \n",
    "0.7159"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLPfe0lBaFL5"
   },
   "source": [
    "In this part, we will try to find the most suitable classification technique for our problem. We want to establish a preliminary performance evaluation to get some first insights on the classification techniques that can be efficiently used to solve the current prediction problem. We will evaluate the baseline performance of various techniques, using the default settings as proposed by the ML library **sklearn**. We will check **9** algorithms:    \n",
    "\n",
    "1) Baseline or Dummy Classifier: used as a reference to evaluate the efficacity of the different algorithms.  \n",
    "2) Logistic Regression  \n",
    "3) Support Vector Classifier  \n",
    "4) Naive Bayes   \n",
    "5) Decision Tree Classifier   \n",
    "6) Random Forest Classifier   \n",
    "7) AdaBoost Classifier   \n",
    "8) Gradient Boosting Classifier   \n",
    "9) XGBoost Classifier   \n",
    "\n",
    "> 🗒 As the dataset is **highly imbalanced** the **_accuracy_score_** is not too informative for the algorithms evaluation. Instead, we want to use **_f1_score_** that offers a tradeoff between precision and recall and **_roc_auc_score_** that measure the ability of a model to distinguish (separate) between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    DummyClassifier(strategy='most_frequent'),\n",
    "    LogisticRegression(),\n",
    "    SVC(probability=True),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(random_state = seed),\n",
    "    AdaBoostClassifier(random_state = seed),\n",
    "    GradientBoostingClassifier(random_state = seed),\n",
    "    XGBClassifier(objective = 'binary:logistic') \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting DummyClassifier is launched\n",
      "fitting DummyClassifier is done in 0.027981042861938477 s\n",
      "fitting LogisticRegression is launched\n",
      "fitting LogisticRegression is done in 1.015674114227295 s\n",
      "fitting GaussianNB is launched\n",
      "fitting GaussianNB is done in 0.10459709167480469 s\n",
      "fitting DecisionTreeClassifier is launched\n",
      "fitting DecisionTreeClassifier is done in 0.5917823314666748 s\n",
      "fitting RandomForestClassifier is launched\n",
      "fitting RandomForestClassifier is done in 15.229117631912231 s\n",
      "fitting AdaBoostClassifier is launched\n",
      "fitting AdaBoostClassifier is done in 6.393838167190552 s\n",
      "fitting GradientBoostingClassifier is launched\n",
      "fitting GradientBoostingClassifier is done in 21.827544927597046 s\n",
      "fitting XGBClassifier is launched\n",
      "[07:42:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fitting XGBClassifier is done in 13.220532894134521 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>DummyClassifier</th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <th>XGBClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Accuracy</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.986331</td>\n",
       "      <td>0.978697</td>\n",
       "      <td>0.988610</td>\n",
       "      <td>0.988610</td>\n",
       "      <td>0.985931</td>\n",
       "      <td>0.986282</td>\n",
       "      <td>0.986941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.985769</td>\n",
       "      <td>0.978705</td>\n",
       "      <td>0.983695</td>\n",
       "      <td>0.984117</td>\n",
       "      <td>0.985329</td>\n",
       "      <td>0.985435</td>\n",
       "      <td>0.985259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">f1_score</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.765451</td>\n",
       "      <td>0.698120</td>\n",
       "      <td>0.802619</td>\n",
       "      <td>0.807627</td>\n",
       "      <td>0.757587</td>\n",
       "      <td>0.764355</td>\n",
       "      <td>0.777253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755435</td>\n",
       "      <td>0.694556</td>\n",
       "      <td>0.717073</td>\n",
       "      <td>0.732227</td>\n",
       "      <td>0.748115</td>\n",
       "      <td>0.750827</td>\n",
       "      <td>0.749178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">roc_auc_score</th>\n",
       "      <th>Train</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.986005</td>\n",
       "      <td>0.979144</td>\n",
       "      <td>0.994434</td>\n",
       "      <td>0.994135</td>\n",
       "      <td>0.985453</td>\n",
       "      <td>0.985247</td>\n",
       "      <td>0.989084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.985336</td>\n",
       "      <td>0.978506</td>\n",
       "      <td>0.921006</td>\n",
       "      <td>0.949716</td>\n",
       "      <td>0.984560</td>\n",
       "      <td>0.983951</td>\n",
       "      <td>0.984092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     DummyClassifier  LogisticRegression  GaussianNB  \\\n",
       "Accuracy      Train         0.967742            0.986331    0.978697   \n",
       "              Test          0.967742            0.985769    0.978705   \n",
       "f1_score      Train         0.000000            0.765451    0.698120   \n",
       "              Test          0.000000            0.755435    0.694556   \n",
       "roc_auc_score Train         0.500000            0.986005    0.979144   \n",
       "              Test          0.500000            0.985336    0.978506   \n",
       "\n",
       "                     DecisionTreeClassifier  RandomForestClassifier  \\\n",
       "Accuracy      Train                0.988610                0.988610   \n",
       "              Test                 0.983695                0.984117   \n",
       "f1_score      Train                0.802619                0.807627   \n",
       "              Test                 0.717073                0.732227   \n",
       "roc_auc_score Train                0.994434                0.994135   \n",
       "              Test                 0.921006                0.949716   \n",
       "\n",
       "                     AdaBoostClassifier  GradientBoostingClassifier  \\\n",
       "Accuracy      Train            0.985931                    0.986282   \n",
       "              Test             0.985329                    0.985435   \n",
       "f1_score      Train            0.757587                    0.764355   \n",
       "              Test             0.748115                    0.750827   \n",
       "roc_auc_score Train            0.985453                    0.985247   \n",
       "              Test             0.984560                    0.983951   \n",
       "\n",
       "                     XGBClassifier  \n",
       "Accuracy      Train       0.986941  \n",
       "              Test        0.985259  \n",
       "f1_score      Train       0.777253  \n",
       "              Test        0.749178  \n",
       "roc_auc_score Train       0.989084  \n",
       "              Test        0.984092  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterables = [[\"Accuracy\", \"f1_score\", \"roc_auc_score\"], [\"Train\", \"Test\"]]\n",
    "ind = pd.MultiIndex.from_product(iterables)\n",
    "\n",
    "metrics = pd.DataFrame(index = ind)\n",
    "for clf in classifiers:\n",
    "    name = str(clf).split('(')[0]\n",
    "    scores = MyFunct.learn(clf, X_train, y_train, X_test, y_test, name)\n",
    "    metrics[name] = scores\n",
    "    \n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results of this preliminary analysis\n",
    "metrics.to_csv(results_path+'preliminary_analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🗒 Given this preliminary analysis, almost all algorithms gives close scores but we can notice that the algorithms that gives the best scores (among the checked algorithms) are the **Logistic regression** and the **XGBoost** classifiers. Hence, we will keep these 2 algorithms for **hyperparameters tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsZxDMpBLIf4"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4w0qSCvOfQf"
   },
   "source": [
    "> 🗒 Logistic regression does not really have any critical hyperparameters to tune. We will not have recourse to regularization because looking at the preliminary analysis done with statsmodels logit, we didn't notice any anomaly. However, it would be practical to evaluate its mean performance using the **k-fold cross validation** technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ7v4WNjLIf5"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Metric : accuracy score ******************\n",
      "\n",
      "fitting LogisticRegression is done in 31.844318866729736s\n",
      "Classifier : LogisticRegression \n",
      "Mean : 0.9863263837272396 \n",
      "Std : 0.0023254987432484654\n",
      "\n",
      "************** Metric : f1 score ******************\n",
      "\n",
      "fitting LogisticRegression is done in 31.69506049156189s\n",
      "Classifier : LogisticRegression \n",
      "Mean : 0.7644634617786664 \n",
      "Std : 0.04540174813269441\n",
      "\n",
      "************** Metric : roc_auc score ******************\n",
      "\n",
      "fitting LogisticRegression is done in 31.698376655578613s\n",
      "Classifier : LogisticRegression \n",
      "Mean : 0.9859623680576236 \n",
      "Std : 0.005786422693895489\n",
      "[09:25:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:26:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:28:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:25:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:26:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:28:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:25:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:26:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:28:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "scorings = ['accuracy', 'f1', 'roc_auc']\n",
    "for s in scorings:\n",
    "    print(f'\\n************** Metric : {s} score ******************\\n')\n",
    "    scores = MyFunct.model_validation(LogisticRegression(),X_train, y_train, cv = 100, scoring = s)\n",
    "    print(f\"Classifier : {scores[0]} \\nMean : {scores[1]} \\nStd : {scores[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:          \n",
    "************** Metric : accuracy score ******************          \n",
    "            \n",
    "fitting LogisticRegression is done in 31.844318866729736s            \n",
    "Classifier : LogisticRegression            \n",
    "Mean : 0.9863263837272396             \n",
    "Std : 0.0023254987432484654             \n",
    "            \n",
    "************** Metric : f1 score ******************              \n",
    "             \n",
    "fitting LogisticRegression is done in 31.69506049156189s             \n",
    "Classifier : LogisticRegression         \n",
    "Mean : 0.7644634617786664            \n",
    "Std : 0.04540174813269441          \n",
    "             \n",
    "************** Metric : roc_auc score ******************         \n",
    "          \n",
    "fitting LogisticRegression is done in 31.698376655578613s         \n",
    "Classifier : LogisticRegression          \n",
    "Mean : 0.9859623680576236          \n",
    "Std : 0.005786422693895489           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ7v4WNjLIf5"
   },
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 🗒 The XGBoost classifier require lot of **hyperparameters** to be tuned. It has about 30 [hyperparameters](https://xgboost.readthedocs.io/en/latest/parameter.html). Given the computation constraints, we will only tune the most important hyperparameters using the **GridSearchCV** technique. We have been inspired by the study established by [Dataiku](https://blog.dataiku.com/narrowing-the-search-which-hyperparameters-really-matter) about the most importnat hyperparameters of well known algorithms. Better results can be obtained with further tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ7v4WNjLIf5"
   },
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OxaPPKdLIf5"
   },
   "outputs": [],
   "source": [
    "estimator = XGBClassifier(objective = 'binary:logistic', seed =seed)\n",
    "params = {'learning_rate':[ 0.01, 0.1, 0.2],\n",
    "          \"n_estimators\": [5, 10, 50, 100],\n",
    "          'max_depth' : [2, 4, 6, 8]}\n",
    "scoring ='f1'\n",
    "\n",
    "best_estimator = MyFunct.model_selection(estimator, X_train, y_train, X_test, y_test, params, scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:         \n",
    "cv =  PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0]))        \n",
    "Fitting 1 folds for each of 48 candidates, totalling 48 fits       \n",
    "Tuning XGBClassifier hyperparameters is done in 692.7736556529999s           \n",
    "          \n",
    "Best Estimator           \n",
    "Best Params         \n",
    "       \n",
    "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}           \n",
    "Best score          \n",
    "         \n",
    "0.7488721804511278           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ7v4WNjLIf5"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorings = ['accuracy', 'f1', 'roc_auc']\n",
    "for s in scorings:\n",
    "    print(f'\\n************** Metric : {s} score ******************\\n')\n",
    "    scores = MyFunct.model_validation(best_estimator, X_train, y_train, cv = cv, scoring = s)\n",
    "    print(f\"Classifier : {scores[0]} \\nMean : {scores[1]} \\nStd : {scores[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:   \n",
    "************** Metric : accuracy score ******************   \n",
    "fitting XGBClassifier is done in 74.6988594532013s   \n",
    "Classifier : XGBClassifier    \n",
    "Mean : 0.9863395178860075    \n",
    "Std : 0.00045854191434234075   \n",
    "  \n",
    "************** Metric : f1 score ******************    \n",
    "fitting XGBClassifier is done in 73.22126317024231s     \n",
    "Classifier : XGBClassifier        \n",
    "Mean : 0.7662324256302723       \n",
    "Std : 0.007874064578495559        \n",
    "        \n",
    "************** Metric : roc_auc score ******************        \n",
    "fitting XGBClassifier is done in 70.36932730674744s      \n",
    "Classifier : XGBClassifier       \n",
    "Mean : 0.985375130047026     \n",
    "Std : 0.0008332235597007553     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wjizX1QLIf6"
   },
   "source": [
    "## Train the chosen models on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting LogisticRegression is done in 1.351513385772705s\n",
      "[19:33:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fitting XGBClassifier is done in 12.032495737075806s\n"
     ]
    }
   ],
   "source": [
    "# train the model on the whole data\n",
    "X1 = append(X_train,X_test,axis=0)\n",
    "y1 = append(y_train,y_test)\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "t0= time()\n",
    "lr_model.fit(X1, y1)\n",
    "name = str(lr_model).split('(')[0]\n",
    "print(f'fitting {name} is done in {time() - t0}s')\n",
    "\n",
    "xgb_model = XGBClassifier(objective = 'binary:logistic', seed =seed, learning_rate = 0.1, max_depth = 4, n_estimators= 100)\n",
    "t0= time()\n",
    "xgb_model.fit(X1, y1)\n",
    "name = str(xgb_model).split('(')[0]\n",
    "print(f'fitting {name} is done in {time() - t0}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf7ofr1eLIf6"
   },
   "source": [
    "# Predict the target of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Tr4CEaPzzbP-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction set (without labels) : (31620, 5)\n"
     ]
    }
   ],
   "source": [
    "# Read data without labels\n",
    "X_without_labels = pd.read_csv(test_filepath)\n",
    "print('Prediction set (without labels) :', X_without_labels.shape)\n",
    "\n",
    "# Convert pandas DataFrames to numpy arrays before using scikit-learn\n",
    "X_without_labels = X_without_labels.values\n",
    "\n",
    "# preprocess\n",
    "X_without_labels  = preprocessor.transform(X_without_labels)\n",
    "\n",
    "# predict\n",
    "name = str(lr_model).split('(')[0]\n",
    "y_pred = lr_model.predict(X_without_labels)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['conversion'])\n",
    "y_pred_df.to_csv(output_path+'conversion_data_test_predictions_'+name+'.csv', index=False)\n",
    "\n",
    "name = str(xgb_model).split('(')[0]\n",
    "y_pred = xgb_model.predict(X_without_labels)\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['conversion'])\n",
    "y_pred_df.to_csv(output_path+'conversion_data_test_predictions_'+name+'.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Classification (3).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
